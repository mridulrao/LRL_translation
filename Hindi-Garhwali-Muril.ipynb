{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06f7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0adba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05da72",
   "metadata": {},
   "source": [
    "# Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a78e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "def process_sentences():\n",
    "    full_text_hindi = []\n",
    "    full_text_garhwali = []\n",
    "    \n",
    "    #process hindi\n",
    "    file_name = \"Hindi_sentences1.docx\"\n",
    "    doc = docx.Document(file_name)\n",
    "    for para in doc.paragraphs:\n",
    "        full_text_hindi.append(para.text)\n",
    "            \n",
    "    #process garhwali\n",
    "    file_name = \"Garhwali_sentences.docx\"\n",
    "    doc = docx.Document(file_name)\n",
    "    for para in doc.paragraphs:\n",
    "        full_text_garhwali.append(para.text)\n",
    "    \n",
    "    \n",
    "    return full_text_hindi, full_text_garhwali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b48c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_text, garhwali_text = process_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34fecb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e1176a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(garhwali_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1d5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a7c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self):\n",
    "        self.encoder = hub.KerasLayer(\"MuRIL_1\", trainable=True)\n",
    "        self.preprocessor = hub.KerasLayer(\"MuRIL_preprocess_1\")\n",
    "        \n",
    "    def generate_tokens(self, sentences):\n",
    "        #list_tensor = tf.convert_to_tensor(sentences)\n",
    "        #processor_output = self.preprocessor(list_tensor)\n",
    "        processor_output = self.preprocessor(tf.constant([sentences]))\n",
    "        encoder_output = self.encoder(processor_output)\n",
    "        \n",
    "        return encoder_output[\"sequence_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc75bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLabelsInputs():\n",
    "    def __init__(self):\n",
    "        self.preprocessor = hub.KerasLayer(\"MuRIL_preprocess_1\")\n",
    "        \n",
    "    def generate_labels_inputs(self, sentences):\n",
    "        #list_tensor = tf.convert_to_tensor(sentences)\n",
    "        #tokens = self.preprocessor(list_tensor)['input_word_ids']\n",
    "        \n",
    "        tokens = self.preprocessor(tf.constant([sentences]))['input_word_ids']\n",
    "        \n",
    "        for i in range(tokens.shape[0]):\n",
    "            token = np.array(tokens[i])\n",
    "            token_list = []\n",
    "            for _ in token:\n",
    "                if _ != 0:\n",
    "                    token_list.append(_)\n",
    "\n",
    "            input_tokens = token_list[:-1]\n",
    "            input_labels = token_list[1:]\n",
    "\n",
    "            for j in range(128):\n",
    "                input_tokens.append(0)\n",
    "                input_labels.append(0)\n",
    "            \n",
    "            #reduce length to 128 \n",
    "            input_labels = tf.ragged.constant([input_labels[:128]]).to_tensor()\n",
    "            input_tokens = tf.ragged.constant([input_tokens[:128]]).to_tensor()\n",
    "            \n",
    "            #create decoder_label and decoder_input\n",
    "            if i == 0:\n",
    "                decoder_label = input_labels\n",
    "                decoder_input = input_tokens\n",
    "            \n",
    "            #concat to decoder_label and decoder_input\n",
    "            else:\n",
    "                decoder_label = tf.concat([decoder_label, input_labels], axis = 0)\n",
    "                decoder_input = tf.concat([decoder_input, input_tokens], axis = 0)\n",
    "                \n",
    "        \n",
    "        return decoder_label, decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d9cb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
    "    \n",
    "    angle_rates = 1 / (10000**depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "    \n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis = -1)\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ce0621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero = True)\n",
    "        self.pos_encoding = positional_encoding(length = 2048, depth = d_model)\n",
    "        \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "934ba63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=768)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76c168fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalMask():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _compute_causal_mask(self, query, value=None):    \n",
    "        q_seq_length = tf.shape(query)[1]\n",
    "        v_seq_length = q_seq_length if value is None else tf.shape(value)[1]\n",
    "        return tf.linalg.band_part(tf.ones((1, q_seq_length, v_seq_length), tf.bool), -1, 0)\n",
    "\n",
    "    def _compute_attention_mask(self, query, value, key=None, attention_mask=None, use_causal_mask=False):\n",
    "        query_mask = getattr(query, \"_keras_mask\", None)\n",
    "        value_mask = getattr(value, \"_keras_mask\", None)\n",
    "        key_mask = getattr(key, \"_keras_mask\", None)\n",
    "        auto_mask = None\n",
    "        if query_mask is not None:\n",
    "            query_mask = tf.cast(query_mask, tf.bool)  # defensive casting\n",
    "            # B = batch size, T = max query length\n",
    "            auto_mask = query_mask[:, :, tf.newaxis]  # shape is [B, T, 1]\n",
    "        if value_mask is not None:\n",
    "            value_mask = tf.cast(value_mask, tf.bool)  # defensive casting\n",
    "            # B = batch size, S == max value length\n",
    "            mask = value_mask[:, tf.newaxis, :]  # shape is [B, 1, S]\n",
    "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
    "        if key_mask is not None:\n",
    "            key_mask = tf.cast(key_mask, tf.bool)  # defensive casting\n",
    "            # B == batch size, S == max key length == max value length\n",
    "            mask = key_mask[:, tf.newaxis, :]  # shape is [B, 1, S]\n",
    "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
    "        if use_causal_mask:\n",
    "            # the shape of the causal mask is [1, T, S]\n",
    "            mask = self._compute_causal_mask(query, value)\n",
    "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
    "        if auto_mask is not None:\n",
    "            # merge attention_mask & automatic mask, to shape [B, T, S]\n",
    "            attention_mask = (\n",
    "                auto_mask\n",
    "                if attention_mask is None\n",
    "                else tf.cast(attention_mask, bool) & auto_mask\n",
    "            )\n",
    "        return attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7675e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention, CausalMask):\n",
    "    def call(self, x, context):\n",
    "        mask = CausalMask()\n",
    "        mask = mask._compute_attention_mask(query=x, \n",
    "                                        value=context, \n",
    "                                        key=context, \n",
    "                                        use_causal_mask=False)\n",
    "        \n",
    "        attn_output = self.mha(query = x,\n",
    "                                key = context,\n",
    "                                value = context,\n",
    "                                attention_mask = mask)\n",
    "        \n",
    "        #self.last_attn_scores = attn_scores\n",
    "        \n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2034211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        mask = CausalMask()\n",
    "        mask = mask._compute_attention_mask(query=x, \n",
    "                                        value=x, \n",
    "                                        key=x, \n",
    "                                        use_causal_mask=True)\n",
    "        \n",
    "        attn_output = self.mha(query = x,\n",
    "                                key = x,\n",
    "                                value = x,\n",
    "                                attention_mask = mask)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d542187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model = 768, dff = 2048, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fccd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):      \n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.causal_self_attention = CausalAttention()\n",
    "        self.cross_attention = CrossAttention()\n",
    "        self.ffn = FeedForward()\n",
    "        \n",
    "    \n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x = x)\n",
    "        x = self.cross_attention(x = x, context = context)\n",
    "        \n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "200768fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size = 197285, \n",
    "                                                 d_model = 768)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "        self.dec_layers = [\n",
    "            DecoderLayer()\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "    def call(self, x, context):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b07e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.decoder = Decoder(num_layers = 4)\n",
    "        self.final_layer = tf.keras.layers.Dense(197285)\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        encoder_output, decoder_input = inputs\n",
    " \n",
    "        decoder_output = self.decoder(decoder_input, encoder_output)\n",
    "        \n",
    "        logits = self.final_layer(decoder_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e590c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = 768\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        \n",
    "        self.warmup_steps = 4000\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype = tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe6c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, \n",
    "                                     beta_1 = 0.9,\n",
    "                                     beta_2 = 0.98,\n",
    "                                     epsilon = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ea50ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(decoder_label, pred_label):\n",
    "    mask = decoder_label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                                            from_logits = True,\n",
    "                                            reduction = 'none')\n",
    "    \n",
    "    loss = loss_object(decoder_label, pred_label)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss.dtype)\n",
    "    \n",
    "    loss *= mask\n",
    "    \n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "add21a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(decoder_label, pred_label):\n",
    "    pred_label = tf.argmax(pred_label, axis = 2)\n",
    "    decoder_label = tf.cast(decoder_label, pred_label.dtype)\n",
    "    \n",
    "    match = decoder_label == pred_label\n",
    "    mask = decoder_label != 0\n",
    "    \n",
    "    match = match & mask\n",
    "    \n",
    "    match = tf.cast(match, dtype = tf.float32)\n",
    "    mask = tf.cast(mask, dtype = tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "704080de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 350/350 [00:48<00:00,  7.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "encoder_output_list = []\n",
    "decoder_input_list = []\n",
    "decoder_label_list = []\n",
    "\n",
    "garhwali_sent = garhwali_text\n",
    "hindi_sent = hindi_text\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder_labels_inputs = DecoderLabelsInputs()\n",
    "\n",
    "for i in tqdm(range(len(hindi_sent))):\n",
    "    encoder_output = encoder.generate_tokens(hindi_sent[i])\n",
    "    encoder_output_list.append(encoder_output)\n",
    "    \n",
    "    decoder_label, decoder_input = decoder_labels_inputs.generate_labels_inputs(garhwali_sent[i])\n",
    "    decoder_input_list.append(decoder_input)\n",
    "    decoder_label_list.append(decoder_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cc207bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 348/348 [00:03<00:00, 113.71it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_1 = encoder_output_list[0]\n",
    "tf_2 = encoder_output_list[1]\n",
    "\n",
    "encoder_output = tf.concat([tf_1, tf_2], axis = 0)\n",
    "\n",
    "for i in tqdm(range(len(encoder_output_list[2:]))):\n",
    "    tf_2 = encoder_output_list[i]\n",
    "    encoder_output = tf.concat([encoder_output, tf_2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e921a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 348/348 [00:00<00:00, 16940.78it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_1 = decoder_input_list[0]\n",
    "tf_2 = decoder_input_list[1]\n",
    "\n",
    "decoder_input = tf.concat([tf_1, tf_2], axis = 0)\n",
    "\n",
    "for i in tqdm(range(len(decoder_input_list[2:]))):\n",
    "    tf_2 = decoder_input_list[i]\n",
    "    decoder_input = tf.concat([decoder_input, tf_2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6949f502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 348/348 [00:00<00:00, 29125.95it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_1 = decoder_label_list[0]\n",
    "tf_2 = decoder_label_list[1]\n",
    "\n",
    "decoder_label = tf.concat([tf_1, tf_2], axis = 0)\n",
    "\n",
    "for i in tqdm(range(len(decoder_label_list[2:]))):\n",
    "    tf_2 = decoder_label_list[i]\n",
    "    decoder_label = tf.concat([decoder_label, tf_2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6e63ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_decoder = TransformerDecoder()\n",
    "transformer_decoder.compile(loss = masked_loss,\n",
    "                          optimizer = optimizer,\n",
    "                          metrics = [masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc699939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 815s 73s/step - loss: 0.8804 - masked_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 748s 65s/step - loss: 0.8774 - masked_accuracy: 0.0059\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 703s 64s/step - loss: 0.8716 - masked_accuracy: 0.1251\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 756s 69s/step - loss: 0.8634 - masked_accuracy: 0.1269\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 763s 69s/step - loss: 0.8536 - masked_accuracy: 0.1124\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 742s 68s/step - loss: 0.8434 - masked_accuracy: 0.1084\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 756s 69s/step - loss: 0.8329 - masked_accuracy: 0.1081\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 735s 67s/step - loss: 0.8212 - masked_accuracy: 0.1081\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 684s 62s/step - loss: 0.8079 - masked_accuracy: 0.1081\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 725s 65s/step - loss: 0.7927 - masked_accuracy: 0.1167\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 691s 62s/step - loss: 0.7758 - masked_accuracy: 0.1618\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 698s 64s/step - loss: 0.7574 - masked_accuracy: 0.1881\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 670s 61s/step - loss: 0.7375 - masked_accuracy: 0.1933\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 652s 59s/step - loss: 0.7161 - masked_accuracy: 0.1930\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 667s 60s/step - loss: 0.6938 - masked_accuracy: 0.1927\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 658s 60s/step - loss: 0.6698 - masked_accuracy: 0.1930\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 657s 60s/step - loss: 0.6449 - masked_accuracy: 0.1930\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 713s 65s/step - loss: 0.6185 - masked_accuracy: 0.1933\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 700s 63s/step - loss: 0.5918 - masked_accuracy: 0.1939\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 682s 62s/step - loss: 0.5645 - masked_accuracy: 0.1939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bbf4d700>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_decoder.fit(x = (encoder_output, decoder_input),\n",
    "                               y = decoder_label,\n",
    "                               epochs = 20,\n",
    "                               verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acb640d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 684s 62s/step - loss: 0.5376 - masked_accuracy: 0.1939\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 692s 63s/step - loss: 0.5118 - masked_accuracy: 0.1939\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 717s 65s/step - loss: 0.4876 - masked_accuracy: 0.1939\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 705s 62s/step - loss: 0.4657 - masked_accuracy: 0.1939\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 706s 63s/step - loss: 0.4470 - masked_accuracy: 0.1939\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 699s 64s/step - loss: 0.4314 - masked_accuracy: 0.1939\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 703s 64s/step - loss: 0.4189 - masked_accuracy: 0.1939\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 739s 65s/step - loss: 0.4088 - masked_accuracy: 0.1939\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 714s 64s/step - loss: 0.4006 - masked_accuracy: 0.1939\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 733s 66s/step - loss: 0.3937 - masked_accuracy: 0.1943\n"
     ]
    }
   ],
   "source": [
    "history = transformer_decoder.fit(x = (encoder_output, decoder_input),\n",
    "                               y = decoder_label,\n",
    "                               epochs = 10,\n",
    "                               verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f7ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
