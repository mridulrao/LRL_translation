{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7339c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8920f4f",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4076b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kawaii/opt/miniconda3/envs/transformers/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f15e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-911387c6837f8b91\n",
      "Reusing dataset parquet (/Users/kawaii/.cache/huggingface/datasets/parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 51.32it/s]\n",
      "100%|████████████████████████████| 1659083/1659083 [00:00<00:00, 2322289.43it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "list_dataset = dataset['train']['translation']\n",
    "english_sent = []\n",
    "hindi_sent = []\n",
    "\n",
    "for i in tqdm(range(len(list_dataset))):\n",
    "    english_sent.append(list_dataset[i]['en'])\n",
    "    hindi_sent.append(list_dataset[i]['hi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2fa45",
   "metadata": {},
   "source": [
    "# MURIL Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34e9703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86fb63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = hub.load(\"https://tfhub.dev/google/MuRIL_preprocess/1\")\n",
    "tokenizer = hub.KerasLayer(preprocessor.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8b2d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentence(tokenizer, string):    \n",
    "    text_input = tf.constant([string])\n",
    "    tokens = tokenizer(text_input)\n",
    "    \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0dc5015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[1475],\n",
       "  [1121],\n",
       "  [172],\n",
       "  [30936]]]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokenized_sentence(tokenizer=tokenizer, string='This is a sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f063e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_tokenized_sentence(tokenizer=tokenizer, string='This is a sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dadb57",
   "metadata": {},
   "source": [
    "# MURIL Encoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08a6184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_input(preprocessor, text_input):\n",
    "    text_input = tf.constant([text_input])\n",
    "    return preprocessor(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46d10c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inp = get_encoder_input(preprocessor=preprocessor, text_input = 'This is a sentence' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18e224",
   "metadata": {},
   "source": [
    "# MURIL Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c330449",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = hub.KerasLayer(\"MuRIL_1\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c449ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_output(encoder, preprocessor_output):\n",
    "    outputs = encoder(preprocessor_output)\n",
    "    \n",
    "    # [batch_size, 768]\n",
    "    pooled_output = outputs[\"pooled_output\"]\n",
    "    \n",
    "    # [batch_size, seq_length, 768]\n",
    "    sequence_output = outputs[\"sequence_output\"]\n",
    "    \n",
    "    return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e817647b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       "array([[[-2.25443160e-03,  2.25501438e-03, -2.43421877e-04, ...,\n",
       "         -1.00110667e-02, -1.94040581e-03, -3.23787844e-03],\n",
       "        [-8.95238761e-03, -3.32291890e-03,  1.57416239e-03, ...,\n",
       "         -2.00973749e-02,  6.45848922e-05, -1.70063751e-03],\n",
       "        [ 3.04290047e-03,  2.23215972e-03, -1.03627553e-03, ...,\n",
       "         -1.47598740e-02,  1.25555089e-04, -1.07521447e-03],\n",
       "        ...,\n",
       "        [-5.47535345e-03,  3.62805766e-03,  2.71241507e-03, ...,\n",
       "         -1.26624415e-02, -1.24334078e-03, -5.86683489e-03],\n",
       "        [-5.77889616e-03,  3.81670101e-03,  2.82584829e-03, ...,\n",
       "         -1.29023548e-02, -1.41847553e-03, -5.85314073e-03],\n",
       "        [-5.71267074e-03,  3.95243615e-03,  2.92897830e-03, ...,\n",
       "         -1.30609758e-02, -1.46879756e-03, -5.95816970e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_encoder_output(encoder=encoder, preprocessor_output=enc_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7de673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737755b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db297747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb72f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e46ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b2392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ebddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 80000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa84a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add [START] and [END] tokens\n",
    "\n",
    "START = tf.argmax(tf.constant(reserved_tokens) == ['START']) + 1\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == ['END']) + 2\n",
    "\n",
    "print(START)\n",
    "print(END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "    \n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e54088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case = True)\n",
    "        self.reserved_tokens = reserved_tokens\n",
    "        self.vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "        \n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "        \n",
    "        #tokenizer signature\n",
    "        self.tokenize.get_concrete_function(tf.TensorSpec(shape=[None], dtype = tf.string))\n",
    "        \n",
    "        self.detokenize.get_concrete_function(tf.TensorSpec(shape=[None, None], dtype = tf.int64))\n",
    "        self.detokenize.get_concrete_function(tf.RaggedTensorSpec(shape=[None, None], dtype = tf.int64))\n",
    "        \n",
    "        self.lookup.get_concrete_function(tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        \n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "        \n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2,-1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self.reserved_tokens, words)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self.vocab_path\n",
    "    \n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self.reserved_tokens)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.eng = CustomTokenizer(reserved_tokens, 'eng_vocab.txt')\n",
    "tokenizers.hin = CustomTokenizer(reserved_tokens, 'hindi_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out the tokenizer\n",
    "print(\"English sentence = \" + english_sent[0])\n",
    "tokens = tokenizers.eng.tokenize([english_sent[0]])\n",
    "print(\"Tokenized text = \" + str(tokens))\n",
    "words = tokenizers.eng.detokenize(tokens)\n",
    "print(\"Detokenized text = \" + str(words.numpy()[0].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15dabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=128\n",
    "def prepare_batch(eng, hindi):\n",
    "    hindi = tokenizers.hin.tokenize(hindi)  # Output is ragged.\n",
    "    hindi = hindi[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    hindi = hindi.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    eng = tokenizers.eng.tokenize(eng)\n",
    "    eng = eng[:, :(MAX_TOKENS+1)]\n",
    "    en_inputs = eng[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    en_labels = eng[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return hindi, en_inputs, en_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_text, english_text, english_label = prepare_batch(english_sent[0], hindi_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e304348",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hindi_text.shape)\n",
    "print(english_text.shape)\n",
    "print(english_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(english_text)\n",
    "print(english_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02787c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
    "    \n",
    "    angle_rate = 1 / (10000**depths)\n",
    "    angle_rad = positions * angle_rate\n",
    "    \n",
    "    pos_encoding = np.concatenate([np.sin(angle_rad), np.cos(angle_rad)], axis = -1)\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(length=2048, depth=768)\n",
    "\n",
    "# Check the shape.\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero = True)\n",
    "        self.pos_encoding = positional_encoding(length = 2048, depth = d_model)\n",
    "        \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_hindi = PositionalEmbedding(vocab_size=tokenizers.hin.get_vocab_size(), d_model=768)\n",
    "embed_eng = PositionalEmbedding(vocab_size=tokenizers.eng.get_vocab_size(), d_model=768)\n",
    "\n",
    "hin_emb = embed_hindi(hindi_text)\n",
    "eng_emb = embed_eng(english_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_emb._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad57ef92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
